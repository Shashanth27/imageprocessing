{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TIzc2-P1UqI"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import nengo\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nengo_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlu0jOtP1UqJ"
      },
      "source": [
        "First we'll load the training data, the MNIST digits/labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdyC_huh1UqJ"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (\n",
        "    test_images,\n",
        "    test_labels,\n",
        ") = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# flatten images\n",
        "train_images = train_images.reshape((train_images.shape[0], -1))\n",
        "test_images = test_images.reshape((test_images.shape[0], -1))\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.imshow(np.reshape(train_images[i], (28, 28)), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(str(train_labels[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29K4-rWw1UqK"
      },
      "outputs": [],
      "source": [
        "with nengo.Network(seed=0) as net:\n",
        "    # set some default parameters for the neurons that will make\n",
        "    # the training progress more smoothly\n",
        "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([100])\n",
        "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
        "    net.config[nengo.Connection].synapse = None\n",
        "    neuron_type = nengo.LIF(amplitude=0.01)\n",
        "\n",
        "    # this is an optimization to improve the training speed,\n",
        "    # since we won't require stateful behaviour in this example\n",
        "    nengo_dl.configure_settings(stateful=False)\n",
        "\n",
        "    # the input node that will be used to feed in input images\n",
        "    inp = nengo.Node(np.zeros(28 * 28))\n",
        "\n",
        "    # add the first convolutional layer\n",
        "    x = nengo_dl.Layer(tf.keras.layers.Conv2D(filters=32, kernel_size=3))(\n",
        "        inp, shape_in=(28, 28, 1)\n",
        "    )\n",
        "    x = nengo_dl.Layer(neuron_type)(x)\n",
        "\n",
        "    # add the second convolutional layer\n",
        "    x = nengo_dl.Layer(tf.keras.layers.Conv2D(filters=64, strides=2, kernel_size=3))(\n",
        "        x, shape_in=(26, 26, 32)\n",
        "    )\n",
        "    x = nengo_dl.Layer(neuron_type)(x)\n",
        "\n",
        "    # add the third convolutional layer\n",
        "    x = nengo_dl.Layer(tf.keras.layers.Conv2D(filters=128, strides=2, kernel_size=3))(\n",
        "        x, shape_in=(12, 12, 64)\n",
        "    )\n",
        "    x = nengo_dl.Layer(neuron_type)(x)\n",
        "\n",
        "    # linear readout\n",
        "    out = nengo_dl.Layer(tf.keras.layers.Dense(units=10))(x)\n",
        "\n",
        "    # we'll create two different output probes, one with a filter\n",
        "    # (for when we're simulating the network over time and\n",
        "    # accumulating spikes), and one without (for when we're\n",
        "    # training the network using a rate-based approximation)\n",
        "    out_p = nengo.Probe(out, label=\"out_p\")\n",
        "    out_p_filt = nengo.Probe(out, synapse=0.1, label=\"out_p_filt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaMxDqZI1UqL"
      },
      "source": [
        "Next we can construct a Simulator for that network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLmzvxpC1UqL"
      },
      "outputs": [],
      "source": [
        "minibatch_size = 200\n",
        "sim = nengo_dl.Simulator(net, minibatch_size=minibatch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK9kPm_a1UqL"
      },
      "source": [
        "Next we set up our training/testing data. We need to incorporate time into this data,\n",
        "since Nengo models (and spiking neural networks in general) always run over time. When\n",
        "training the model we'll be using a rate-based approximation, so we can run that for a\n",
        "single timestep.  But when testing the model we'll be using the spiking neuron models,\n",
        "so we need to run the model for multiple timesteps in order to collect the spike data\n",
        "over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXV-NzhF1UqL"
      },
      "outputs": [],
      "source": [
        "# add single timestep to training data\n",
        "train_images = train_images[:, None, :]\n",
        "train_labels = train_labels[:, None, None]\n",
        "\n",
        "# when testing our network with spiking neurons we will need to run it\n",
        "# over time, so we repeat the input/target data for a number of\n",
        "# timesteps.\n",
        "n_steps = 30\n",
        "test_images = np.tile(test_images[:, None, :], (1, n_steps, 1))\n",
        "test_labels = np.tile(test_labels[:, None, None], (1, n_steps, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m0rw-d51UqM"
      },
      "source": [
        "In order to quantify the network's performance we'll use a classification accuracy\n",
        "function (the percentage of test images classified correctly). We're using a custom\n",
        "function here, because we only want to evaluate the output from the network on the final\n",
        "timestep (as we are simulating the network over time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcMh39TO1UqM"
      },
      "outputs": [],
      "source": [
        "def classification_accuracy(y_true, y_pred):\n",
        "    return tf.metrics.sparse_categorical_accuracy(y_true[:, -1], y_pred[:, -1])\n",
        "\n",
        "\n",
        "# note that we use `out_p_filt` when testing (to reduce the spike noise)\n",
        "sim.compile(loss={out_p_filt: classification_accuracy})\n",
        "print(\n",
        "    \"Accuracy before training:\",\n",
        "    sim.evaluate(test_images, {out_p_filt: test_labels}, verbose=0)[\"loss\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTO_wg841UqM"
      },
      "source": [
        "Now we are ready to train the network.  For training we'll use the standard categorical\n",
        "cross entropy loss function, and the RMSprop optimizer.\n",
        "\n",
        "In order to keep this example relatively quick we are going to download some pretrained\n",
        "weights.  However, if you'd like to run the training yourself set `do_training=True`\n",
        "below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18P1Govf1UqM"
      },
      "outputs": [],
      "source": [
        "do_training = False\n",
        "if do_training:\n",
        "    # run training\n",
        "    sim.compile(\n",
        "        optimizer=tf.optimizers.RMSprop(0.001),\n",
        "        loss={out_p: tf.losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
        "    )\n",
        "    sim.fit(train_images, {out_p: train_labels}, epochs=10)\n",
        "\n",
        "    # save the parameters to file\n",
        "    sim.save_params(\"./mnist_params\")\n",
        "else:\n",
        "    # download pretrained weights\n",
        "    urlretrieve(\n",
        "        \"https://drive.google.com/uc?export=download&\"\n",
        "        \"id=1l5aivQljFoXzPP5JVccdFXbOYRv3BCJR\",\n",
        "        \"mnist_params.npz\",\n",
        "    )\n",
        "\n",
        "    # load parameters\n",
        "    sim.load_params(\"./mnist_params\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDvn1r2p1UqN"
      },
      "source": [
        "Now we can check the classification accuracy again, with the trained parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Cbmgvy1UqN"
      },
      "outputs": [],
      "source": [
        "sim.compile(loss={out_p_filt: classification_accuracy})\n",
        "print(\n",
        "    \"Accuracy after training:\",\n",
        "    sim.evaluate(test_images, {out_p_filt: test_labels}, verbose=0)[\"loss\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrwSYnFO1UqN"
      },
      "source": [
        "We can see that the spiking neural network is achieving ~99% accuracy, which is what we\n",
        "would expect for MNIST. `n_steps` could be increased to further improve performance,\n",
        "since we would get a more accurate measure of each spiking neuron's output.\n",
        "\n",
        "We can also plot some example outputs from the network, to see how it is performing over\n",
        "time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oft1JoNh1UqN"
      },
      "outputs": [],
      "source": [
        "data = sim.predict(test_images[:minibatch_size])\n",
        "\n",
        "for i in range(5):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(test_images[i, 0].reshape((28, 28)), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(tf.nn.softmax(data[out_p_filt][i]))\n",
        "    plt.legend([str(i) for i in range(10)], loc=\"upper left\")\n",
        "    plt.xlabel(\"timesteps\")\n",
        "    plt.ylabel(\"probability\")\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkLD7Ynl1UqO"
      },
      "outputs": [],
      "source": [
        "sim.close()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}